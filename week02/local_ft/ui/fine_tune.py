import gradio as gr
from core.fine_tune_manager import ft_manager
from core.data_manager import data_manager
from ui.html_templates import nav_html
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
import re
import numpy as np
from typing import List, Tuple
import os

def parse_training_logs(logs: str) -> Tuple[List[float], List[float], List[int]]:
    """è§£æžè®­ç»ƒæ—¥å¿—ï¼Œæå–losså’Œæ­¥æ•°"""
    lines = logs.split('\n')
    steps = []
    losses = []
    eval_losses = []
    
    for line in lines:
        train_match = re.search(r"'loss':\s*([\d.]+)", line)
        if train_match:
            step_match = re.search(r'(\d+)/\d+', line)
            if step_match:
                step = int(step_match.group(1))
                loss = float(train_match.group(1))
                steps.append(step)
                losses.append(loss)
        
        eval_match = re.search(r"'eval_loss':\s*([\d.]+)", line)
        if eval_match:
            eval_loss = float(eval_match.group(1))
            eval_losses.append(eval_loss)
    
    return losses, eval_losses, steps

def create_loss_plot(logs: str) -> str:
    """åˆ›å»ºlossæ›²çº¿å›¾"""
    try:
        losses, eval_losses, steps = parse_training_logs(logs)
        
        if not losses:
            return None
        
        plt.figure(figsize=(12, 6))
        plt.style.use('seaborn-v0_8')
        
        if losses and steps:
            plt.plot(steps, losses, 'b-', label='Training Loss', linewidth=2, alpha=0.8)
        
        if eval_losses:
            eval_steps = steps[-len(eval_losses):] if len(eval_losses) <= len(steps) else steps
            plt.plot(eval_steps, eval_losses, 'r-', label='Evaluation Loss', linewidth=2, alpha=0.8)
        
        plt.xlabel('Steps', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.title('Training Progress', fontsize=14, fontweight='bold')
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plot_path = "logs/loss_plot.png"
        os.makedirs("logs", exist_ok=True)
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        return plot_path
    except Exception as e:
        print(f"åˆ›å»ºlosså›¾è¡¨å¤±è´¥: {e}")
        return None

def start_training_wrapper(
    model, train_type, datasets_text, torch_dtype,
    num_train_epochs, per_device_train_batch_size, per_device_eval_batch_size,
    learning_rate, gradient_accumulation_steps,
    lora_rank, lora_alpha, target_modules,
    eval_steps, save_steps, save_total_limit, logging_steps,
    max_length, warmup_ratio, dataloader_num_workers,
    cuda_visible_devices, system, model_author, model_name
):
    """å¯åŠ¨è®­ç»ƒ"""
    datasets = [ds.strip() for ds in datasets_text.split('\n') if ds.strip()]
    
    config = {
        "model": model,
        "train_type": train_type,
        "datasets": datasets,
        "torch_dtype": torch_dtype,
        "num_train_epochs": num_train_epochs,
        "per_device_train_batch_size": per_device_train_batch_size,
        "per_device_eval_batch_size": per_device_eval_batch_size,
        "learning_rate": learning_rate,
        "gradient_accumulation_steps": gradient_accumulation_steps,
        "lora_rank": lora_rank,
        "lora_alpha": lora_alpha,
        "target_modules": target_modules,
        "eval_steps": eval_steps,
        "save_steps": save_steps,
        "save_total_limit": save_total_limit,
        "logging_steps": logging_steps,
        "max_length": max_length,
        "warmup_ratio": warmup_ratio,
        "dataloader_num_workers": dataloader_num_workers,
        "cuda_visible_devices": cuda_visible_devices,
        "system": system,
        "model_author": model_author,
        "model_name": model_name
    }
    
    return ft_manager.start_training(config)

def update_training_status():
    """æ›´æ–°è®­ç»ƒçŠ¶æ€"""
    status = ft_manager.get_training_status()
    logs = ft_manager.get_training_logs()
    plot_path = create_loss_plot(logs)
    
    status_text = f"ðŸ”„ çŠ¶æ€: {status['status']}\nðŸ“Š æ—¥å¿—è¡Œæ•°: {status['log_count']}\nâš¡ è¿›ç¨‹å­˜æ´»: {status['process_alive']}"
    
    return status_text, logs, plot_path

def get_fine_tune_block():
    """å¾®è°ƒç•Œé¢"""
    with gr.Blocks(
        theme=gr.themes.Soft(),
        css="""
        .block {
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(255, 255, 255, 0.1);
        }
        """
    ) as demo:
        gr.HTML(nav_html)
        
        gr.Markdown("# ðŸš€ æ¨¡åž‹å¾®è°ƒ", elem_classes=["text-center"])
        
        with gr.Tab("âš™ï¸ å¾®è°ƒé…ç½®"):
            with gr.Row():
                with gr.Column(scale=1):
                    # åŸºç¡€é…ç½®
                    with gr.Accordion("ðŸ”§ åŸºç¡€é…ç½®", open=True):
                        model = gr.Textbox(
                            label="æ¨¡åž‹åç§°",
                            value="Qwen/Qwen2.5-7B-Instruct",
                            placeholder="ä¾‹å¦‚: Qwen/Qwen2.5-7B-Instruct"
                        )
                        train_type = gr.Dropdown(
                            choices=["lora", "full"],
                            label="è®­ç»ƒç±»åž‹",
                            value="lora"
                        )
                        torch_dtype = gr.Dropdown(
                            choices=["bfloat16", "float16", "float32"],
                            label="æ•°æ®ç±»åž‹",
                            value="bfloat16"
                        )
                        cuda_visible_devices = gr.Textbox(
                            label="CUDAè®¾å¤‡",
                            value="0",
                            placeholder="ä¾‹å¦‚: 0 æˆ– 0,1"
                        )
                    
                    # æ•°æ®é›†é…ç½®
                    with gr.Accordion("ðŸ“Š æ•°æ®é›†é…ç½®", open=True):
                        datasets_text = gr.Textbox(
                            label="æ•°æ®é›†åˆ—è¡¨ (æ¯è¡Œä¸€ä¸ª)",
                            value="AI-ModelScope/alpaca-gpt4-data-zh#500\nAI-ModelScope/alpaca-gpt4-data-en#500",
                            lines=4,
                            placeholder="æ¯è¡Œè¾“å…¥ä¸€ä¸ªæ•°æ®é›†"
                        )
                
                with gr.Column(scale=1):
                    # è®­ç»ƒå‚æ•°
                    with gr.Accordion("ðŸŽ¯ è®­ç»ƒå‚æ•°", open=True):
                        with gr.Row():
                            num_train_epochs = gr.Number(label="è®­ç»ƒè½®æ•°", value=1, minimum=1)
                            learning_rate = gr.Number(label="å­¦ä¹ çŽ‡", value=1e-4, step=1e-5)
                        with gr.Row():
                            per_device_train_batch_size = gr.Number(label="è®­ç»ƒæ‰¹æ¬¡å¤§å°", value=1, minimum=1)
                            per_device_eval_batch_size = gr.Number(label="è¯„ä¼°æ‰¹æ¬¡å¤§å°", value=1, minimum=1)
                        gradient_accumulation_steps = gr.Number(label="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°", value=16, minimum=1)
                        max_length = gr.Number(label="æœ€å¤§é•¿åº¦", value=2048, minimum=128)
                    
                    # LoRAå‚æ•°
                    with gr.Accordion("ðŸ”— LoRAå‚æ•°", open=True):
                        lora_rank = gr.Number(label="LoRA Rank", value=8, minimum=1)
                        lora_alpha = gr.Number(label="LoRA Alpha", value=32, minimum=1)
                        target_modules = gr.Textbox(label="ç›®æ ‡æ¨¡å—", value="all-linear")
            
            with gr.Row():
                with gr.Column():
                    # å…¶ä»–å‚æ•°
                    with gr.Accordion("ðŸ“ å…¶ä»–å‚æ•°", open=False):
                        eval_steps = gr.Number(label="è¯„ä¼°æ­¥æ•°", value=50, minimum=1)
                        save_steps = gr.Number(label="ä¿å­˜æ­¥æ•°", value=50, minimum=1)
                        save_total_limit = gr.Number(label="ä¿å­˜æ€»æ•°é™åˆ¶", value=2, minimum=1)
                        logging_steps = gr.Number(label="æ—¥å¿—æ­¥æ•°", value=5, minimum=1)
                        warmup_ratio = gr.Number(label="é¢„çƒ­æ¯”ä¾‹", value=0.05, minimum=0, maximum=1)
                        dataloader_num_workers = gr.Number(label="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°", value=4, minimum=0)
                
                with gr.Column():
                    # æ¨¡åž‹ä¿¡æ¯
                    with gr.Accordion("ðŸ·ï¸ æ¨¡åž‹ä¿¡æ¯", open=True):
                        system = gr.Textbox(
                            label="ç³»ç»Ÿæç¤º",
                            value="You are a helpful assistant.",
                            lines=2
                        )
                        model_author = gr.Textbox(label="æ¨¡åž‹ä½œè€…", value="YourName")
                        model_name = gr.Textbox(label="æ¨¡åž‹åç§°", value="CustomModel")
            
            # æŽ§åˆ¶æŒ‰é’®
            with gr.Row():
                start_btn = gr.Button("ðŸš€ å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
                stop_btn = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="stop", size="lg")
        
        with gr.Tab("ðŸ“ˆ è®­ç»ƒç›‘æŽ§"):
            with gr.Row():
                with gr.Column(scale=1):
                    status_display = gr.Textbox(
                        label="è®­ç»ƒçŠ¶æ€",
                        lines=5,
                        interactive=False
                    )
                    refresh_btn = gr.Button("ðŸ”„ åˆ·æ–°çŠ¶æ€", variant="secondary")
                
                with gr.Column(scale=2):
                    loss_plot = gr.Image(label="Lossæ›²çº¿", type="filepath")
            
            logs_display = gr.Textbox(
                label="è®­ç»ƒæ—¥å¿— (æœ€è¿‘50è¡Œ)",
                lines=20,
                interactive=False,
                max_lines=50
            )
        
        # äº‹ä»¶ç»‘å®š
        start_btn.click(
            fn=start_training_wrapper,
            inputs=[
                model, train_type, datasets_text, torch_dtype,
                num_train_epochs, per_device_train_batch_size, per_device_eval_batch_size,
                learning_rate, gradient_accumulation_steps,
                lora_rank, lora_alpha, target_modules,
                eval_steps, save_steps, save_total_limit, logging_steps,
                max_length, warmup_ratio, dataloader_num_workers,
                cuda_visible_devices, system, model_author, model_name
            ],
            outputs=[status_display]
        )
        
        stop_btn.click(
            fn=ft_manager.stop_training,
            outputs=[status_display]
        )
        
        refresh_btn.click(
            fn=update_training_status,
            outputs=[status_display, logs_display, loss_plot]
        )
        
        # è‡ªåŠ¨åˆ·æ–° - ä½¿ç”¨æ–°ç‰ˆGradioè¯­æ³•
        gr.Timer(5).tick(
            fn=update_training_status,
            outputs=[status_display, logs_display, loss_plot]
        )
    
    return demo