import gradio as gr
import subprocess
import os
from pathlib import Path
from ui.html_templates import nav_html

def quantize_model(
    model_path: str, 
    output_name: str, 
    quant_bits: int, 
    quant_method: str,
    dataset: str,
    max_length: int,
    num_samples: int
) -> str:
    """é‡åŒ–æ¨¡å‹"""
    if not model_path:
        return "âŒ è¯·è¾“å…¥æ¨¡å‹è·¯å¾„"
    
    if not output_name:
        return "âŒ è¯·è¾“å…¥è¾“å‡ºåç§°"
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("quantized_models", exist_ok=True)
        
        # æ„å»ºé‡åŒ–å‘½ä»¤
        cmd = [
            "swift", "export",
            "--model", model_path,
            "--quant_bits", str(quant_bits),
            "--quant_method", quant_method,
            "--output_dir", f"quantized_models/{output_name}",
            "--max_length", str(max_length)
        ]
        
        # æ·»åŠ æ•°æ®é›†å‚æ•°
        if dataset and dataset != "æ— ":
            cmd.extend(["--dataset", dataset])
            cmd.extend(["--num_samples", str(num_samples)])
        
        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=7200  # 2å°æ—¶è¶…æ—¶
        )
        
        if result.returncode == 0:
            return f"âœ… æ¨¡å‹é‡åŒ–æˆåŠŸï¼\nè¾“å‡ºè·¯å¾„: quantized_models/{output_name}\n\n{result.stdout}"
        else:
            return f"âŒ æ¨¡å‹é‡åŒ–å¤±è´¥:\n{result.stderr}"
    
    except subprocess.TimeoutExpired:
        return "âŒ é‡åŒ–è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å¤§å°å’Œç³»ç»Ÿèµ„æº"
    except Exception as e:
        return f"âŒ é‡åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

def get_quantized_models() -> list:
    """è·å–å·²é‡åŒ–çš„æ¨¡å‹åˆ—è¡¨"""
    quantized_dir = Path("quantized_models")
    if not quantized_dir.exists():
        return []
    
    models = []
    for model_dir in quantized_dir.iterdir():
        if model_dir.is_dir():
            models.append(model_dir.name)
    
    return sorted(models, reverse=True)

def get_merged_models_for_quant() -> list:
    """è·å–å¯ç”¨äºé‡åŒ–çš„åˆå¹¶æ¨¡å‹"""
    merged_dir = Path("merged_models")
    models = ["Qwen/Qwen2.5-7B-Instruct", "Qwen/Qwen2.5-14B-Instruct"]  # é¢„å®šä¹‰æ¨¡å‹
    
    if merged_dir.exists():
        for model_dir in merged_dir.iterdir():
            if model_dir.is_dir():
                models.append(f"merged_models/{model_dir.name}")
    
    return models

def delete_quantized_model(model_name: str) -> str:
    """åˆ é™¤é‡åŒ–æ¨¡å‹"""
    if not model_name:
        return "âŒ è¯·é€‰æ‹©è¦åˆ é™¤çš„æ¨¡å‹"
    
    try:
        model_path = Path("quantized_models") / model_name
        if model_path.exists():
            import shutil
            shutil.rmtree(model_path)
            return f"âœ… æˆåŠŸåˆ é™¤é‡åŒ–æ¨¡å‹: {model_name}"
        else:
            return f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_name}"
    except Exception as e:
        return f"âŒ åˆ é™¤å¤±è´¥: {str(e)}"

def get_model_info(model_path: str) -> str:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    if not model_path:
        return "è¯·é€‰æ‹©æ¨¡å‹"
    
    try:
        path = Path("quantized_models") / model_path
        if not path.exists():
            return "æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨"
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        total_size = 0
        file_count = 0
        for file_path in path.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
                file_count += 1
        
        size_mb = total_size / (1024 * 1024)
        size_gb = size_mb / 1024
        
        info = f"ğŸ“ æ¨¡å‹è·¯å¾„: {path}\n"
        info += f"ğŸ“Š æ–‡ä»¶æ•°é‡: {file_count}\n"
        info += f"ğŸ’¾ æ¨¡å‹å¤§å°: {size_mb:.2f} MB ({size_gb:.2f} GB)\n"
        info += f"ğŸ“… åˆ›å»ºæ—¶é—´: {path.stat().st_mtime}"
        
        return info
    except Exception as e:
        return f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}"

def get_quantization_block():
    """é‡åŒ–ç•Œé¢"""
    with gr.Blocks(
        theme=gr.themes.Soft(),
        css="""
        .block {
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(255, 255, 255, 0.1);
        }
        """
    ) as demo:
        gr.HTML(nav_html)
        
        gr.Markdown("# ğŸ—œï¸ æ¨¡å‹é‡åŒ–", elem_classes=["text-center"])
        
        with gr.Tab("âš¡ é‡åŒ–æ¨¡å‹"):
            gr.Markdown("### å°†æ¨¡å‹é‡åŒ–ä¸ºINT8/INT4æ ¼å¼ä»¥å‡å°‘å†…å­˜å ç”¨")
            
            with gr.Row():
                with gr.Column(scale=1):
                    # åŸºç¡€é…ç½®
                    with gr.Accordion("ğŸ”§ åŸºç¡€é…ç½®", open=True):
                        model_dropdown = gr.Dropdown(
                            choices=get_merged_models_for_quant(),
                            label="é€‰æ‹©æ¨¡å‹",
                            interactive=True,
                            value="Qwen/Qwen2.5-7B-Instruct"
                        )
                        refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨")
                        
                        output_name = gr.Textbox(
                            label="è¾“å‡ºåç§°",
                            placeholder="é‡åŒ–åçš„æ¨¡å‹åç§°",
                            value="quantized_model"
                        )
                
                with gr.Column(scale=1):
                    # é‡åŒ–å‚æ•°
                    with gr.Accordion("âš™ï¸ é‡åŒ–å‚æ•°", open=True):
                        quant_bits = gr.Dropdown(
                            choices=[4, 8],
                            label="é‡åŒ–ä½æ•°",
                            value=8
                        )
                        quant_method = gr.Dropdown(
                            choices=["bnb", "gptq", "awq"],
                            label="é‡åŒ–æ–¹æ³•",
                            value="bnb"
                        )
                        max_length = gr.Number(
                            label="æœ€å¤§é•¿åº¦",
                            value=2048,
                            minimum=128
                        )
                
                with gr.Column(scale=1):
                    # æ ¡å‡†æ•°æ®é›†
                    with gr.Accordion("ğŸ“Š æ ¡å‡†æ•°æ®é›†", open=True):
                        dataset = gr.Dropdown(
                            choices=["æ— ", "AI-ModelScope/alpaca-gpt4-data-zh", "AI-ModelScope/alpaca-gpt4-data-en"],
                            label="æ ¡å‡†æ•°æ®é›†",
                            value="AI-ModelScope/alpaca-gpt4-data-zh"
                        )
                        num_samples = gr.Number(
                            label="æ ·æœ¬æ•°é‡",
                            value=128,
                            minimum=1
                        )
            
            quantize_btn = gr.Button("ğŸš€ å¼€å§‹é‡åŒ–", variant="primary", size="lg")
            
            quantize_result = gr.Textbox(
                label="é‡åŒ–ç»“æœ",
                lines=12,
                interactive=False
            )
        
        with gr.Tab("ğŸ“ ç®¡ç†é‡åŒ–æ¨¡å‹"):
            gr.Markdown("### ç®¡ç†å·²é‡åŒ–çš„æ¨¡å‹")
            
            with gr.Row():
                with gr.Column(scale=2):
                    quantized_models_dropdown = gr.Dropdown(
                        choices=get_quantized_models(),
                        label="å·²é‡åŒ–çš„æ¨¡å‹",
                        interactive=True
                    )
                    
                with gr.Column(scale=1):
                    refresh_quantized_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                    delete_quantized_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤æ¨¡å‹", variant="stop")
            
            manage_result = gr.Textbox(
                label="æ“ä½œç»“æœ",
                lines=3,
                interactive=False
            )
            
            # æ¨¡å‹ä¿¡æ¯
            with gr.Accordion("ğŸ“Š æ¨¡å‹ä¿¡æ¯", open=True):
                model_info = gr.Textbox(
                    label="æ¨¡å‹è¯¦æƒ…",
                    lines=8,
                    interactive=False,
                    placeholder="é€‰æ‹©æ¨¡å‹æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"
                )
        
        with gr.Tab("ğŸ“‹ é‡åŒ–è¯´æ˜"):
            gr.Markdown("""
            ## ğŸ¯ é‡åŒ–æ–¹æ³•è¯´æ˜
            
            ### INT8é‡åŒ– (æ¨è)
            - **ç²¾åº¦æŸå¤±**: è¾ƒå°
            - **å‹ç¼©æ¯”**: ~50%
            - **æ¨ç†é€Ÿåº¦**: ä¸­ç­‰æå‡
            - **é€‚ç”¨åœºæ™¯**: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
            
            ### INT4é‡åŒ–
            - **ç²¾åº¦æŸå¤±**: è¾ƒå¤§
            - **å‹ç¼©æ¯”**: ~75%
            - **æ¨ç†é€Ÿåº¦**: æ˜¾è‘—æå‡
            - **é€‚ç”¨åœºæ™¯**: èµ„æºå—é™ç¯å¢ƒ
            
            ## ğŸ”§ é‡åŒ–æ–¹æ³•å¯¹æ¯”
            
            | æ–¹æ³• | ç‰¹ç‚¹ | é€‚ç”¨æ¨¡å‹ |
            |------|------|----------|
            | **BNB** | ç®€å•æ˜“ç”¨ï¼Œå…¼å®¹æ€§å¥½ | å¤§éƒ¨åˆ†æ¨¡å‹ |
            | **GPTQ** | é«˜ç²¾åº¦ï¼Œéœ€è¦æ ¡å‡†æ•°æ® | Transformeræ¨¡å‹ |
            | **AWQ** | æ¿€æ´»æ„ŸçŸ¥ï¼Œç²¾åº¦æœ€é«˜ | æ–°ç‰ˆæœ¬æ¨¡å‹ |
            
            ## ğŸ’¡ ä½¿ç”¨å»ºè®®
            
            1. **é¦–æ¬¡é‡åŒ–**: å»ºè®®ä½¿ç”¨INT8 + BNBæ–¹æ³•
            2. **ç²¾åº¦è¦æ±‚é«˜**: é€‰æ‹©AWQæ–¹æ³•
            3. **èµ„æºå—é™**: ä½¿ç”¨INT4é‡åŒ–
            4. **æ ¡å‡†æ•°æ®**: é€‰æ‹©ä¸ç›®æ ‡ä»»åŠ¡ç›¸å…³çš„æ•°æ®é›†
            """)
        
        # äº‹ä»¶ç»‘å®š
        quantize_btn.click(
            fn=quantize_model,
            inputs=[
                model_dropdown, output_name, quant_bits, 
                quant_method, dataset, max_length, num_samples
            ],
            outputs=[quantize_result]
        )
        
        refresh_models_btn.click(
            fn=get_merged_models_for_quant,
            outputs=[model_dropdown]
        )
        
        refresh_quantized_btn.click(
            fn=get_quantized_models,
            outputs=[quantized_models_dropdown]
        )
        
        delete_quantized_btn.click(
            fn=delete_quantized_model,
            inputs=[quantized_models_dropdown],
            outputs=[manage_result]
        )
        
        quantized_models_dropdown.change(
            fn=get_model_info,
            inputs=[quantized_models_dropdown],
            outputs=[model_info]
        )
    
    return demo