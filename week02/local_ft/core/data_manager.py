import os
import json
import shutil
from pathlib import Path
from typing import List, Dict, Any

class DataManager:
    def __init__(self):
        self.data_dir = Path("training_data")
        self.data_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.data_dir / "uploaded").mkdir(exist_ok=True)
        (self.data_dir / "processed").mkdir(exist_ok=True)
        
    def upload_dataset(self, files: List[str], dataset_name: str) -> str:
        """ä¸Šä¼ æ•°æ®é›†æ–‡ä»¶"""
        if not files:
            return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶"
        
        if not dataset_name:
            return "è¯·è¾“å…¥æ•°æ®é›†åç§°"
        
        try:
            dataset_dir = self.data_dir / "uploaded" / dataset_name
            dataset_dir.mkdir(exist_ok=True)
            
            uploaded_files = []
            for file_path in files:
                if os.path.exists(file_path):
                    file_name = os.path.basename(file_path)
                    dest_path = dataset_dir / file_name
                    shutil.copy2(file_path, dest_path)
                    uploaded_files.append(file_name)
            
            return f"âœ… æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶åˆ°æ•°æ®é›† '{dataset_name}'"
        
        except Exception as e:
            return f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}"
    
    def create_custom_dataset(self, instructions: str, inputs: str, outputs: str, dataset_name: str) -> str:
        """åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†"""
        try:
            inst_list = [line.strip() for line in instructions.split('\n') if line.strip()]
            input_list = [line.strip() for line in inputs.split('\n') if line.strip()]
            output_list = [line.strip() for line in outputs.split('\n') if line.strip()]
            
            if len(inst_list) != len(input_list) or len(inst_list) != len(output_list):
                return "âŒ æŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºçš„è¡Œæ•°å¿…é¡»ç›¸åŒ"
            
            data = []
            for i in range(len(inst_list)):
                data.append({
                    "instruction": inst_list[i],
                    "input": input_list[i],
                    "output": output_list[i]
                })
            
            output_path = self.data_dir / "processed" / f"{dataset_name}.jsonl"
            with open(output_path, 'w', encoding='utf-8') as f:
                for item in data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            return f"âœ… æˆåŠŸåˆ›å»ºåŒ…å« {len(data)} æ¡æ•°æ®çš„æ•°æ®é›†: {dataset_name}"
        
        except Exception as e:
            return f"âŒ åˆ›å»ºå¤±è´¥: {str(e)}"
    
    def get_available_datasets(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ•°æ®é›†"""
        datasets = []
        
        # å¤„ç†åçš„æ•°æ®é›†
        processed_dir = self.data_dir / "processed"
        if processed_dir.exists():
            for file in processed_dir.glob("*.jsonl"):
                datasets.append(f"custom:{file.stem}")
        
        # é¢„å®šä¹‰æ•°æ®é›†
        predefined = [
            "AI-ModelScope/alpaca-gpt4-data-zh",
            "AI-ModelScope/alpaca-gpt4-data-en", 
            "swift/self-cognition",
            "AI-ModelScope/chinese-medical-dialogue",
            "AI-ModelScope/code-alpaca-zh"
        ]
        datasets.extend(predefined)
        
        return datasets
    
    def preview_dataset(self, dataset_path: str, num_samples: int = 3) -> str:
        """é¢„è§ˆæ•°æ®é›†"""
        try:
            if dataset_path.startswith("custom:"):
                file_name = dataset_path.replace("custom:", "")
                file_path = self.data_dir / "processed" / f"{file_name}.jsonl"
                
                if not file_path.exists():
                    return "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨"
                
                samples = []
                with open(file_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= num_samples:
                            break
                        if line.strip():
                            samples.append(json.loads(line))
                
                preview_text = f"ğŸ“‹ æ•°æ®é›†é¢„è§ˆ ({len(samples)} æ¡æ ·æœ¬):\n\n"
                for i, sample in enumerate(samples):
                    preview_text += f"æ ·æœ¬ {i+1}:\n"
                    preview_text += f"æŒ‡ä»¤: {sample.get('instruction', 'N/A')}\n"
                    preview_text += f"è¾“å…¥: {sample.get('input', 'N/A')}\n"
                    preview_text += f"è¾“å‡º: {sample.get('output', 'N/A')}\n"
                    preview_text += "-" * 50 + "\n"
                
                return preview_text
            else:
                return f"ğŸ“‹ é¢„å®šä¹‰æ•°æ®é›†: {dataset_path}\nè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£äº†è§£æ•°æ®æ ¼å¼"
        
        except Exception as e:
            return f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
    
    def delete_dataset(self, dataset_name: str) -> str:
        """åˆ é™¤æ•°æ®é›†"""
        try:
            if dataset_name.startswith("custom:"):
                file_name = dataset_name.replace("custom:", "")
                file_path = self.data_dir / "processed" / f"{file_name}.jsonl"
                
                if file_path.exists():
                    file_path.unlink()
                    return f"âœ… æˆåŠŸåˆ é™¤æ•°æ®é›†: {dataset_name}"
                else:
                    return "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨"
            else:
                return "âŒ æ— æ³•åˆ é™¤é¢„å®šä¹‰æ•°æ®é›†"
        
        except Exception as e:
            return f"âŒ åˆ é™¤å¤±è´¥: {str(e)}"

# å…¨å±€å®ä¾‹
data_manager = DataManager()